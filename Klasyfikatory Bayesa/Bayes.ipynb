{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 7 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie dwóch wersji naiwnego klasyfikatora Bayesa.\n",
        "* W pierwszej wersji należy dokonać dyskretyzacji danych - przedział wartości każdego atrybutu dzielimy na cztery równe przedziały i każdej ciągłej wartości atrybutu przypisujemy wartość dyskretną wynikająca z przynależności do danego przedziału.\n",
        "* W drugiej wersji wartości likelihood wyliczamy z rozkładów normalnych o średnich i odchyleniach standardowych wynikających z wartości atrybutów.\n",
        "Trening i test należy przeprowadzić dla zbioru Iris, tak jak w przypadku zadania z drzewem klasyfikacyjnym. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania klasyfikatorów dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Dyskretyzacja danych - **0.5 pkt**\n",
        "* Implementacja funkcji rozkładu normalnego o zadanej średniej i odchyleniu standardowym. - **0.5 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych dyskretnych. - **2.0 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych ciągłych. - **2.5 pkt**\n",
        "* Przeprowadzenie eksperymentów, wnioski i sposób ich prezentacji. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self, num_of_bins: int):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "        self.num_of_bins = num_of_bins\n",
        "\n",
        "    def build_classifier(self, train_features: np.ndarray, train_classes: np.ndarray) -> None:\n",
        "        self._set_priors(train_classes)\n",
        "        discrete_features = self.data_discretization(train_features, self.num_of_bins)\n",
        "        self._set_likelihoods(discrete_features, train_classes)\n",
        "\n",
        "    def _set_priors(self, train_classes: np.ndarray) -> None:\n",
        "        for single_class in np.unique(train_classes):\n",
        "            class_prob = np.sum(train_classes == single_class) / len(train_classes)\n",
        "            self.priors[single_class] = class_prob\n",
        "\n",
        "    def _set_likelihoods(self, discrete_train_features: np.ndarray, train_classes: np.ndarray) -> None:\n",
        "        alpha = 0.5\n",
        "\n",
        "        for single_class in np.unique(train_classes):\n",
        "            self.likelihoods[single_class] = {}\n",
        "            features = discrete_train_features[train_classes == single_class]\n",
        "\n",
        "            num_of_features = discrete_train_features.shape[1]\n",
        "            for feature_index in range(num_of_features):\n",
        "                self.likelihoods[single_class][feature_index] = {}\n",
        "                feature = features[:, feature_index]\n",
        "\n",
        "                for bin_num in range(1, self.num_of_bins + 1):\n",
        "                    feature_for_single_bin = feature[feature == bin_num]\n",
        "                    feature_prob = (len(feature_for_single_bin) + alpha\n",
        "                                    ) / (len(feature) + alpha)\n",
        "\n",
        "                    self.likelihoods[single_class][feature_index][bin_num] = feature_prob\n",
        "\n",
        "    @staticmethod\n",
        "    def data_discretization(data: np.ndarray, num_of_bins: int = 4) -> np.ndarray:\n",
        "        copied_data = data.copy()\n",
        "        num_of_features = data.shape[1]\n",
        "        for feature_index in range(num_of_features):\n",
        "            feature_values = data[:, feature_index]\n",
        "            bins_ranges = np.linspace(np.min(feature_values), np.max(feature_values) + 1e-10, num_of_bins + 1)\n",
        "            feature_discrete = np.digitize(feature_values, bins_ranges)\n",
        "            copied_data[:, feature_index] = feature_discrete\n",
        "        return copied_data\n",
        "\n",
        "    def predict(self, samples: np.ndarray) -> np.ndarray:\n",
        "        discrete_samples = self.data_discretization(samples, self.num_of_bins)\n",
        "\n",
        "        predictions = []\n",
        "        for sample in discrete_samples:\n",
        "            posterior_probabilities = {}\n",
        "            for single_class in self.priors:\n",
        "                posterior_probability = self.priors[single_class]\n",
        "\n",
        "                for feature_index, bin_num in enumerate(sample):\n",
        "                    posterior_probability *= self.likelihoods[single_class][feature_index][bin_num]\n",
        "\n",
        "                posterior_probabilities[single_class] = posterior_probability\n",
        "            predicted_class = max(posterior_probabilities, key=lambda k: posterior_probabilities[k])\n",
        "            predictions.append(predicted_class)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "    def build_classifier(self, train_features: np.ndarray, train_classes: np.ndarray) -> None:\n",
        "        self._set_priors(train_classes)\n",
        "        self._set_likelihoods(train_features, train_classes)\n",
        "\n",
        "    def _set_priors(self, train_classes: np.ndarray) -> None:\n",
        "        for single_class in np.unique(train_classes):\n",
        "            class_prob = np.sum(train_classes == single_class) / len(train_classes)\n",
        "            self.priors[single_class] = class_prob\n",
        "\n",
        "    def _set_likelihoods(self, train_features: np.ndarray, train_classes: np.ndarray) -> None:\n",
        "        for single_class in np.unique(train_classes):\n",
        "            self.likelihoods[single_class] = {}\n",
        "            features = train_features[train_classes == single_class]\n",
        "\n",
        "            num_of_features = train_features.shape[1]\n",
        "            for feature_index in range(num_of_features):\n",
        "                self.likelihoods[single_class][feature_index] = {}\n",
        "                feature = features[:, feature_index]\n",
        "\n",
        "                mean = feature.mean()\n",
        "                std = feature.std()\n",
        "\n",
        "                if std == 0:\n",
        "                    std += 1e-10\n",
        "\n",
        "                self.likelihoods[single_class][feature_index] = {'mean': mean, 'std': std}\n",
        "\n",
        "    @staticmethod\n",
        "    def normal_dist_density(x, mean: float, std: float) -> float:\n",
        "        exp = np.exp(-((x - mean)**2 / (2 * std**2)))\n",
        "        return (1 / std * (np.sqrt(2 * np.pi))) * exp\n",
        "\n",
        "    def predict(self, samples: np.ndarray) -> np.ndarray:\n",
        "        predictions = []\n",
        "        for sample in samples:\n",
        "            posterior_likelihoods = {}\n",
        "            for single_class in self.priors:\n",
        "                posterior_likelihood = np.log(self.priors[single_class])\n",
        "\n",
        "                for feature_index, feature_value in enumerate(sample):\n",
        "                    mean = self.likelihoods[single_class][feature_index]['mean']\n",
        "                    std = self.likelihoods[single_class][feature_index]['std']\n",
        "                    normal_dist_dens = self.normal_dist_density(feature_value, mean, std)\n",
        "                    posterior_likelihood += np.log(normal_dist_dens)\n",
        "\n",
        "                posterior_likelihoods[single_class] = posterior_likelihood\n",
        "            predicted_class = max(posterior_likelihoods, key=lambda k: posterior_likelihoods[k])\n",
        "            predictions.append(predicted_class)\n",
        "\n",
        "        return np.array(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testy dla podanego zbioru testowego i trenującego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Classifier\n",
            "Predicted classes:\t[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n",
            "Aktual classes:   \t[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n",
            "\n",
            "Gaussian Naive Bayes Classifier\n",
            "Predicted classes:\t[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n",
            "Aktual classes:   \t[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n"
          ]
        }
      ],
      "source": [
        "bayes_classifier = NaiveBayes(num_of_bins=4)\n",
        "bayes_classifier.build_classifier(x_train, y_train)\n",
        "print(\"Naive Bayes Classifier\")\n",
        "print(\"Predicted classes:\\t\", end=\"\")\n",
        "print(bayes_classifier.predict(x_test))\n",
        "print(\"Aktual classes:   \\t\", end=\"\")\n",
        "print(y_test)\n",
        "\n",
        "gausian_bayer_classifer = GaussianNaiveBayes()\n",
        "gausian_bayer_classifer.build_classifier(x_train, y_train)\n",
        "print(\"\\nGaussian Naive Bayes Classifier\")\n",
        "print(\"Predicted classes:\\t\", end=\"\")\n",
        "print(gausian_bayer_classifer.predict(x_test))\n",
        "print(\"Aktual classes:   \\t\", end=\"\")\n",
        "print(y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
